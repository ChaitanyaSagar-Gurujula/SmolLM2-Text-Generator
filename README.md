---
title: SmolLM2 Text Generator
emoji: ðŸ”¥
colorFrom: indigo
colorTo: gray
sdk: docker
pinned: false
---


# SmolLM2 GPT Text Generator

This project is a web-based application for generating text continuations using GPT-based models. Users can input text through a simple UI, and the application will respond with a continuation generated by the underlying GPT model. Model is based on the SmolLM2 architecture. It is a ~135M parameter model. 

## HuggingFace Space

[SmolLM2 GPT Text Generator Hugging Face Space App](https://huggingface.co/spaces/sagargurujula/SmolLM2-Text-Generator)

## Features

- **User-friendly interface**: A clean and responsive web page to input text and display results.
- **Real-time generation**: Text is processed and displayed almost instantly.

## Files in the Project

### `index.html`

The front-end of the application, providing a user interface for input and output.

- Includes a text area for input and a submit button to initiate text generation.
- Displays the original input and the generated continuation in separate sections.
- Features CSS styling for a clean look and responsive design.

### `model.py`

The backend model that handles text generation.

- Uses a SmolLM2 model for generating text.
- Processes the input text and generates a continuation.
- Model is based on the SmolLM2 architecture. It is a ~135M parameter model. 
- Model is trained on the Cosmopedia V2 dataset in Hugging Face.
- **Model details**: 
   ``` 
   GPT(
   (token_embedding): Embedding(49152, 576)
   (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
         (self_attn): CausalSelfAttention(
         (cq_attn): Linear(in_features=576, out_features=576, bias=False)
         (ckv_attn): Linear(in_features=576, out_features=384, bias=False)
         (c_proj): Linear(in_features=576, out_features=576, bias=False)
         (rope): RotaryPositionalEmbeddings()
         )
         (input_layernorm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
         (post_attention_layernorm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
         (mlp): LlamaMLP(
         (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
         (up_proj): Linear(in_features=576, out_features=1536, bias=False)
         (down_proj): Linear(in_features=1536, out_features=576, bias=False)
         (act_fn): SiLU()
         )
      )
   )
   (final_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
   (lm_head): Linear(in_features=576, out_features=49152, bias=False)
   )
   ```
- **Model Summary**:
   ``` 
   =========================================================================================================
   Layer (type:depth-idx)                                  Output Shape              Param #
   =========================================================================================================
   GPT                                                     [1, 2048, 49152]          --
   â”œâ”€Embedding: 1-1                                        [1, 2048, 576]            28,311,552
   â”œâ”€ModuleList: 1-2                                       --                        --
   â”‚    â””â”€LlamaDecoderLayer: 2-1                           [1, 2048, 576]            --
   â”‚    â”‚    â””â”€RMSNorm: 3-1                                [1, 2048, 576]            576
   â”‚    â”‚    â””â”€CausalSelfAttention: 3-2                    [1, 2048, 576]            884,736
   â”‚    â”‚    â””â”€RMSNorm: 3-3                                [1, 2048, 576]            576
   â”‚    â”‚    â””â”€LlamaMLP: 3-4                               [1, 2048, 576]            2,654,208
   â”‚    â””â”€LlamaDecoderLayer: 2-2                           [1, 2048, 576]            --
   â”‚    â”‚    â””â”€RMSNorm: 3-5                                [1, 2048, 576]            576
   â”‚    â”‚    â””â”€CausalSelfAttention: 3-6                    [1, 2048, 576]            884,736
   â”‚    â”‚    â””â”€RMSNorm: 3-7                                [1, 2048, 576]            576
   â”‚    â”‚    â””â”€LlamaMLP: 3-8                               [1, 2048, 576]            2,654,208
   â”‚    â””â”€LlamaDecoderLayer: 2-3                           [1, 2048, 576]            --
   â”‚    â”‚    â””â”€RMSNorm: 3-9                                [1, 2048, 576]            576
   â”‚    â”‚    â””â”€CausalSelfAttention: 3-10                   [1, 2048, 576]            884,736
   â”‚    â”‚    â””â”€RMSNorm: 3-11                               [1, 2048, 576]            576
   â”‚    â”‚    â””â”€LlamaMLP: 3-12                              [1, 2048, 576]            2,654,208
   â”‚    â””â”€LlamaDecoderLayer: 2-4                           [1, 2048, 576]            --
   â”‚    â”‚    â””â”€RMSNorm: 3-13                               [1, 2048, 576]            576
   â”‚    â”‚    â””â”€CausalSelfAttention: 3-14                   [1, 2048, 576]            884,736
   â”‚    â”‚    â””â”€RMSNorm: 3-15                               [1, 2048, 576]            576
   ...
   Input size (MB): 0.02
   Forward/backward pass size (MB): 3938.45
   Params size (MB): 651.31
   Estimated Total Size (MB): 4589.77
   =========================================================================================================
   ```
### `app.py`

The Flask-based server that bridges the front-end and back-end.

- Defines API endpoints for interacting with the model (`/generate/`).
- Handles POST requests from the front-end, passes input to the model, and returns generated text.
- Includes error handling to manage invalid inputs or server issues.

## Prerequisites

- **Python**: Ensure Python 3.7 or higher is installed.
- **Flask**: Install Flask using `pip install flask`.
- **Model dependencies**: Install necessary packages for running the GPT model (`transformers`, `torch`, etc., if applicable).

Note: Refer `requirements.txt` file for dependencies.

## Setup and Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd <repository-folder> # if you are in the root folder
   ```

2. Install the dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the server:
   ```bash
   python app.py
   ```

4. Open your browser and navigate to `http://localhost:8080` to use the application.

## Logs
- [Complete Training Logs ](training_log.txt)
``` Logs
2025-02-01 01:45:41,986 - INFO - Total Parameters: 134,515,008
2025-02-01 01:45:41,986 - INFO - Trainable Parameters: 134,515,008
2025-02-01 01:45:58,592 - INFO - Epoch 0, Step 0, Loss: 11.355476, Best Loss: 11.355476
2025-02-01 01:46:09,209 - INFO - Epoch 0, Step 1, Loss: 11.328232, Best Loss: 11.328232
2025-02-01 01:46:19,635 - INFO - Epoch 0, Step 2, Loss: 11.316939, Best Loss: 11.316939
2025-02-01 01:46:27,642 - INFO - Epoch 0, Step 3, Loss: 11.357376, Best Loss: 11.316939
2025-02-01 01:46:38,116 - INFO - Epoch 0, Step 4, Loss: 11.305080, Best Loss: 11.305080
2025-02-01 01:46:46,129 - INFO - Epoch 0, Step 5, Loss: 11.334230, Best Loss: 11.305080
2025-02-01 01:46:54,134 - INFO - Epoch 0, Step 6, Loss: 11.323222, Best Loss: 11.305080
2025-02-01 01:47:02,250 - INFO - Epoch 0, Step 7, Loss: 11.343060, Best Loss: 11.305080
2025-02-01 01:47:15,049 - INFO - Epoch 0, Step 8, Loss: 10.165360, Best Loss: 10.165360
2025-02-01 01:47:23,077 - INFO - Epoch 0, Step 9, Loss: 10.364421, Best Loss: 10.165360
.
.
.
.
2025-02-01 14:15:42,483 - INFO - Epoch 0, Step 4995, Loss: 4.230450, Best Loss: 4.102502
2025-02-01 14:15:50,508 - INFO - Epoch 0, Step 4996, Loss: 4.634490, Best Loss: 4.102502
2025-02-01 14:15:58,539 - INFO - Epoch 0, Step 4997, Loss: 4.260272, Best Loss: 4.102502
2025-02-01 14:16:06,572 - INFO - Epoch 0, Step 4998, Loss: 4.510636, Best Loss: 4.102502
2025-02-01 14:16:14,623 - INFO - Epoch 0, Step 4999, Loss: 4.590177, Best Loss: 4.102502
2025-02-01 14:16:16,566 - INFO - Step 5000 Prompt: Once upon a time 
 Generated Token: [28, 281, 253, 28249, 1666, 1217, 2591, 28, 665, 436, 253, 1528, 282, 701, 617, 4161, 281, 253, 1528, 282, 701, 617, 4161, 281, 253, 1528, 282, 701, 617, 4161, 281, 253, 1528, 282, 701, 617, 4161, 281, 253, 1528, 282, 701, 617, 4161, 281, 253, 1528, 282, 701, 30] 
 Prediction: , in a faraway land called Earth, there was a group of people who lived in a group of people who lived in a group of people who lived in a group of people who lived in a group of people who lived in a group of people.
2025-02-01 14:16:24,595 - INFO - Epoch 0, Step 5000, Loss: 4.321911, Best Loss: 4.102502
2025-02-01 14:16:27,485 - INFO - Max iterations reached. Training stopped.
2025-02-01 14:16:27,506 - INFO - Training completed!
2025-02-01 14:16:27,515 - INFO - Final Loss: 4.321911
2025-02-01 14:16:27,515 - INFO - Best Loss Achieved: 4.102502
2025-02-01 14:16:27,515 - INFO - Best Model Saved To: /kaggle/working/best_model.pth
2025-02-01 14:16:27,515 - INFO - Checpoint Model Saved To: /kaggle/working/checkpoint_model.pth
2025-02-01 14:39:27,845 - INFO - Resuming from epoch 0, step 5001, best loss 4.102502
2025-02-01 14:39:27,847 - INFO - Total Parameters: 134,515,008
2025-02-01 14:39:27,847 - INFO - Trainable Parameters: 134,515,008
2025-02-01 14:39:48,115 - INFO - Epoch 0, Step 5001, Loss: 4.682718, Best Loss: 4.102502
2025-02-01 14:39:56,142 - INFO - Epoch 0, Step 5002, Loss: 4.552681, Best Loss: 4.102502
2025-02-01 14:40:04,168 - INFO - Epoch 0, Step 5003, Loss: 4.262938, Best Loss: 4.102502
2025-02-01 14:40:12,198 - INFO - Epoch 0, Step 5004, Loss: 4.162656, Best Loss: 4.102502
2025-02-01 14:40:20,229 - INFO - Epoch 0, Step 5005, Loss: 4.282134, Best Loss: 4.102502
```


## Hugging Face App Screenshots

![Example 1](Hugging%20Face%20Space%20Example%201.png)

![Example 2](Hugging%20Face%20Space%20Example%202.png)

